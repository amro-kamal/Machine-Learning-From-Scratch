{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural net from scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsdDXe7vF_mU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cEQ2CKq4y6M",
        "colab_type": "text"
      },
      "source": [
        "Simple 2 layers Neural Netwok\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok1HZ58-qRAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class two_layers_neural_net:\n",
        "  def __init__(self,input,y):\n",
        "    self.input=input\n",
        "    self.input_shape=self.input.shape        #n[0] x m .....3x8\n",
        "    self.y=y                    #n[l] x 1   ............2x8\n",
        "    self.weights1   = np.random.rand(self.input.shape[1],3) \n",
        "    self.weights2   = np.random.rand(3,1) \n",
        "    layer_weights=np.random.rand(self.nodes_per_layer[0],self.input_shape[0])*0.01\n",
        "    layer_bias=np.zeros((self.nodes_per_layer[0],1))\n",
        "    self.weights.append(layer_weights)\n",
        "    self.b.append(layer_bias)\n",
        "    print( 'layer 1 weights shape is',layer_weights.shape)\n",
        "    print( 'layer 1 weights  is \\n',layer_weights)\n",
        "\n",
        "    print( 'layer 1 bias shape is',layer_bias.shape)\n",
        "    print( 'layer 1  bias  is',layer_bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OBFaYYEy41J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.random.seed(90) \n",
        "\n",
        "# r=np.random.rand(10)\n",
        "# r1=np.random.rand(10)\n",
        "# print(r)\n",
        "# print(r1)\n",
        "\n",
        "# np.random.seed(90) \n",
        "\n",
        "# r=np.random.rand(10)\n",
        "# r1=np.random.rand(10)\n",
        "# print(r)\n",
        "# print(r1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyvoJSUQ5lJx",
        "colab_type": "text"
      },
      "source": [
        "Simple Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8Fww03p04o5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "      \n",
        "        \n",
        "\n",
        "class neural_net:\n",
        "  def __init__(self,input,nodes_per_layer,activations,y):\n",
        "#     x shape is  (3, 8)\n",
        "#     X  is \n",
        "#      [[ 3  4  3 33 23 53  3 33]\n",
        "#       [ 4  1  9 42 44 64 64 34]\n",
        "#       [ 2  2  4  2 25 72  2 42]]\n",
        "#     y shape is (2, 8)\n",
        "#     y  is \n",
        "#       [[1 0 1 0 0 1 1 1]\n",
        "#        [1 1 0 0 1 0 1 0]]\n",
        "\n",
        "    self.input=input\n",
        "    self.input_shape=self.input.shape        #n[0] x m .....3x8\n",
        "    self.num_samples=self.input.shape[1]           # 8\n",
        "    self.nodes_per_layer=nodes_per_layer\n",
        "    self.activations=activations\n",
        "    self.y=y                    #n[l] x 1   ............2x8\n",
        "    self.weights=[]\n",
        "    self.b=[]\n",
        "    layer_weights=np.random.rand(self.nodes_per_layer[0],self.input_shape[0])*0.01\n",
        "    layer_bias=np.zeros((self.nodes_per_layer[0],1))\n",
        "    self.weights.append(layer_weights)\n",
        "    self.b.append(layer_bias)\n",
        "    print( 'layer 1 weights shape is',layer_weights.shape)\n",
        "    print( 'layer 1 weights  is \\n',layer_weights)\n",
        "\n",
        "    print( 'layer 1 bias shape is',layer_bias.shape)\n",
        "    print( 'layer 1  bias  is',layer_bias)\n",
        "\n",
        "\n",
        "    for i in range(self.nodes_per_layer.shape[0]-1):    #[4,3,2]\n",
        "      layer_weights=np.random.rand(self.nodes_per_layer[i+1],self.nodes_per_layer[i])*0.01\n",
        "      self.weights.append(layer_weights)\n",
        "      layer_bias=np.zeros((self.nodes_per_layer[i+1],1))\n",
        "      self.b.append(layer_bias)\n",
        "      print( 'layer ',str(i+2) ,' weights shape is',layer_weights.shape)\n",
        "      print( 'layer ',str(i+2) ,' weights  is \\n',layer_weights)\n",
        "\n",
        "      print( 'layer ',str(i+2) ,' bias shape is',layer_bias.shape)\n",
        "      print( 'layer ',str(i+2) ,' bias  is \\n',layer_bias)\n",
        "\n",
        "    \n",
        "\n",
        "  def forward_propagation(self):\n",
        "    print('forward propadation')\n",
        "\n",
        "    self.a=[]         #[4,1],[3,1],[2,1]\n",
        "    z=np.dot(self.weights[0] , self.input) + self.b[0]\n",
        "    self.layer_output =self.Activation(z,self.activations[0]) \n",
        "    self.a.append(self.layer_output)\n",
        "    # print('layer 1 output \\n',self.layer_output)\n",
        "\n",
        "    for i in range(1,self.nodes_per_layer.shape[0]):    #[4,3,2]\n",
        "      z=np.dot(self.weights[i] , self.layer_output) + self.b[i]\n",
        "      self.layer_output=self.Activation(z,self.activations[i]) \n",
        "      self.a.append(self.layer_output)\n",
        "      # print('layer ' , str(i+1) ,' output \\n',self.layer_output)\n",
        "\n",
        "    # print('a= \\n',self.a)\n",
        "\n",
        "\n",
        "  def back_propagation(self):\n",
        "    print('backpropagation')\n",
        "    self.dz=np.empty_like(np.asarray(self.a))\n",
        "    self.dW=np.empty_like(np.asarray(self.weights))\n",
        "    self.db=np.empty_like(np.asarray(self.b))\n",
        "\n",
        "    self.dz[-1]=(self.a[-1]-self.y)\n",
        "    self.dW[-1]=np.dot(self.dz[-1] , self.a[-2].T) * (1/self.num_samples)\n",
        "    self.db[-1]=np.sum(self.dz[-1],axis=0,keepdims=True).T * (1/self.num_samples)\n",
        "\n",
        "    for i in range(-(self.nodes_per_layer.shape[0]-2),1):    #[4,3,2]\n",
        "      i=-i             #1,0\n",
        "      self.dz[i]=np.dot(self.weights[i+1].T ,self.dz[i+1]) * ( self.a[i]*(1-self.a[i]) )\n",
        "      self.db[i]=np.sum(self.dz[i],axis=0,keepdims=True).T * (1/self.num_samples)\n",
        "\n",
        "      if(i!=0):\n",
        "        self.dW[i]=np.dot(self.dz[i] , self.a[i-1].T) * (1/self.num_samples)\n",
        "\n",
        "      elif(i==0):\n",
        "        self.dW[i]=np.dot(self.dz[i] , self.input.T) * (1/self.num_samples)\n",
        "\n",
        "\n",
        "    self.gradient_discent(self.weights,self.dW,0.01)\n",
        "\n",
        "  def gradient_discent(self,w,dw,lr):\n",
        "    self.weights=w-lr * dw\n",
        "\n",
        "    \n",
        "  def train(self,epoch):\n",
        "    self.loss=[]\n",
        "    for i in range(epoch):\n",
        "      print ('epoch ',str(i+1))\n",
        "      self.forward_propagation()\n",
        "      self.loss.append(self.cross_entropy_loss(self.a[-1],self.y))\n",
        "      self.back_propagation()\n",
        "      print('loss= ',self.loss[i])\n",
        "    plt.plot(self.loss)\n",
        "    plt.ylabel('loss')\n",
        "    plt.show()\n",
        "\n",
        "    return self.loss,self.weights\n",
        "\n",
        "  def sigmoid(self,z):\n",
        "    s= 1/(1 + np.exp(-z)) \n",
        "    return s\n",
        "\n",
        "  def Relu(self,z):\n",
        "    return np.maximum(0,z)\n",
        "\n",
        "  def tanh(self,z):\n",
        "    return (np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z))\n",
        "\n",
        "  def softmax(self,y):\n",
        "    exps = np.exp(y)\n",
        "    return exps / np.sum(exps)\n",
        "\n",
        "  def Activation(self,z,method='sigmoid'):\n",
        "    \n",
        "      if (method=='sigmoid'):\n",
        "        return self.sigmoid(z)\n",
        "      elif(method=='tanh'):\n",
        "        return delf.tanh(z)\n",
        "      elif (method=='Relu'):\n",
        "        return delf.Relu(z)\n",
        "        \n",
        "    \n",
        "\n",
        "  def Activation_derivative(self,a,activation='sigmoid'):\n",
        "    if (activation=='sigmoid'):\n",
        "      return a(1-a)\n",
        "    elif (activation=='tanh'):\n",
        "      return 1-a\n",
        "    elif (activation=='Relu'):\n",
        "      return np.minimum(-np.maximum(0,z),1)\n",
        "\n",
        "  def cross_entropy_loss(self,a, y):\n",
        "    return - np.mean(\n",
        "        np.multiply(y, np.log(a)) + np.multiply((1-y), np.log(1-a)))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSWorzCBgOmx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d34a5053-1d54-4471-cc2d-e96f3ca81e80"
      },
      "source": [
        "nn.weights[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.19658121, 0.89751183, 0.02228812],\n",
              "       [0.9295796 , 0.88364449, 0.88045702]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIFw7R6Egqxt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3ad4c79-7f4f-418e-fba6-384e7c4f48bf"
      },
      "source": [
        "nn.weights[2].T.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ux34K6Tgw3Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "e08095ea-e035-449b-c6a1-3cd91df1955e"
      },
      "source": [
        "nn.dz[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.20888674,  0.79107993, -0.20876549,  0.79134245,  0.79134245,\n",
              "        -0.20865755, -0.20877853, -0.20865755],\n",
              "       [-0.06710439, -0.06709321,  0.93300826,  0.93314711, -0.06685289,\n",
              "         0.93314711, -0.06701088,  0.93314711]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEfdeL_YftvD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce69065f-0896-4ab1-812b-2a6cf8978c9b"
      },
      "source": [
        "# (nn.a[2]*(1-nn.a[2])).shape\n",
        "np.dot(nn.weights[2].T ,nn.dz[2]).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g1syxpCJITr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "457e47ff-7bed-40df-fdde-81f260212b75"
      },
      "source": [
        "\n",
        "(nn.a[2]*(1-nn.a[2])).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPA1aH_61M6C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "outputId": "3a0566aa-d706-41c6-82e8-44c1347b1f4a"
      },
      "source": [
        "X=np.array([[3,4,2] ,[4,1,2],[3,9,4],[33,42,2],[23,44,25],[53,64,72],[3,64,2],[33,34,42]]).T\n",
        "print('x shape is ',X.shape)\n",
        "\n",
        "print('X  is \\n',X)\n",
        "# print('X.T  is \\n',X.T)\n",
        "\n",
        "# w=np.random.randn(4,3)\n",
        "# print('w = \\n',w)\n",
        "y=np.array([[1,1],[0,1],[1,0],[0,0],[0,1],[1,0],[1,1],[1,0]]).T\n",
        "print('y shape is \\n',y.shape)\n",
        "print('y  is \\n',y)\n",
        "\n",
        "# print('w dot X= \\n',w.dot(X.T))\n",
        "\n",
        "nn=neural_net(X,np.array([4,3,2]),np.array(['sigmoid','sigmoid','sigmoid']),y)   #  (self,input,nodes_per_layer,activations,y):\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x shape is  (3, 8)\n",
            "X  is \n",
            " [[ 3  4  3 33 23 53  3 33]\n",
            " [ 4  1  9 42 44 64 64 34]\n",
            " [ 2  2  4  2 25 72  2 42]]\n",
            "y shape is \n",
            " (2, 8)\n",
            "y  is \n",
            " [[1 0 1 0 0 1 1 1]\n",
            " [1 1 0 0 1 0 1 0]]\n",
            "layer 1 weights shape is (4, 3)\n",
            "layer 1 weights  is \n",
            " [[0.00388423 0.00707655 0.00959098]\n",
            " [0.00794527 0.00347766 0.0021746 ]\n",
            " [0.00842831 0.00782939 0.00466523]\n",
            " [0.00492321 0.00754987 0.00878896]]\n",
            "layer 1 bias shape is (4, 1)\n",
            "layer 1  bias  is [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "layer  2  weights shape is (3, 4)\n",
            "layer  2  weights  is \n",
            " [[0.00688052 0.00856036 0.00759095 0.00304715]\n",
            " [0.00912847 0.0006284  0.00188361 0.00398751]\n",
            " [0.00395264 0.00761594 0.00371905 0.00449489]]\n",
            "layer  2  bias shape is (3, 1)\n",
            "layer  2  bias  is \n",
            " [[0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "layer  3  weights shape is (2, 3)\n",
            "layer  3  weights  is \n",
            " [[0.00453729 0.00011229 0.00051757]\n",
            " [0.00073397 0.00400627 0.00897952]]\n",
            "layer  3  bias shape is (2, 1)\n",
            "layer  3  bias  is \n",
            " [[0.]\n",
            " [0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M-Ab4og3ORf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3445202e-169f-4da1-cdb1-407cec89dd7c"
      },
      "source": [
        "nn.train(100)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  1\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6929883926572735\n",
            "epoch  2\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6929296703511896\n",
            "epoch  3\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6928711707285004\n",
            "epoch  4\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6928128929048596\n",
            "epoch  5\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6927548359998131\n",
            "epoch  6\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6926969991367771\n",
            "epoch  7\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6926393814430183\n",
            "epoch  8\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6925819820496335\n",
            "epoch  9\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6925248000915294\n",
            "epoch  10\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6924678347074018\n",
            "epoch  11\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6924110850397163\n",
            "epoch  12\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6923545502346882\n",
            "epoch  13\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6922982294422623\n",
            "epoch  14\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6922421218160943\n",
            "epoch  15\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6921862265135309\n",
            "epoch  16\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6921305426955895\n",
            "epoch  17\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6920750695269404\n",
            "epoch  18\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6920198061758872\n",
            "epoch  19\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6919647518143476\n",
            "epoch  20\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6919099056178348\n",
            "epoch  21\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.691855266765439\n",
            "epoch  22\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6918008344398082\n",
            "epoch  23\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6917466078271308\n",
            "epoch  24\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6916925861171161\n",
            "epoch  25\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6916387685029768\n",
            "epoch  26\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6915851541814111\n",
            "epoch  27\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6915317423525836\n",
            "epoch  28\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6914785322201095\n",
            "epoch  29\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6914255229910344\n",
            "epoch  30\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.691372713875819\n",
            "epoch  31\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6913201040883195\n",
            "epoch  32\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6912676928457719\n",
            "epoch  33\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6912154793687744\n",
            "epoch  34\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6911634628812688\n",
            "epoch  35\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6911116426105259\n",
            "epoch  36\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6910600177871266\n",
            "epoch  37\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6910085876449453\n",
            "epoch  38\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6909573514211348\n",
            "epoch  39\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6909063083561073\n",
            "epoch  40\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.69085545769352\n",
            "epoch  41\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6908047986802575\n",
            "epoch  42\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6907543305664156\n",
            "epoch  43\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6907040526052857\n",
            "epoch  44\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6906539640533382\n",
            "epoch  45\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6906040641702071\n",
            "epoch  46\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6905543522186729\n",
            "epoch  47\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6905048274646484\n",
            "epoch  48\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6904554891771617\n",
            "epoch  49\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6904063366283417\n",
            "epoch  50\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6903573690934017\n",
            "epoch  51\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6903085858506248\n",
            "epoch  52\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6902599861813478\n",
            "epoch  53\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6902115693699468\n",
            "epoch  54\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6901633347038216\n",
            "epoch  55\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.690115281473381\n",
            "epoch  56\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6900674089720273\n",
            "epoch  57\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6900197164961428\n",
            "epoch  58\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6899722033450735\n",
            "epoch  59\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6899248688211155\n",
            "epoch  60\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6898777122294997\n",
            "epoch  61\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6898307328783788\n",
            "epoch  62\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6897839300788108\n",
            "epoch  63\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6897373031447467\n",
            "epoch  64\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6896908513930151\n",
            "epoch  65\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6896445741433086\n",
            "epoch  66\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6895984707181699\n",
            "epoch  67\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6895525404429772\n",
            "epoch  68\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6895067826459312\n",
            "epoch  69\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6894611966580411\n",
            "epoch  70\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6894157818131104\n",
            "epoch  71\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6893705374477239\n",
            "epoch  72\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6893254629012342\n",
            "epoch  73\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.689280557515748\n",
            "epoch  74\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6892358206361128\n",
            "epoch  75\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6891912516099037\n",
            "epoch  76\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6891468497874101\n",
            "epoch  77\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6891026145216232\n",
            "epoch  78\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6890585451682221\n",
            "epoch  79\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6890146410855613\n",
            "epoch  80\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6889709016346581\n",
            "epoch  81\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.688927326179179\n",
            "epoch  82\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6888839140854278\n",
            "epoch  83\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6888406647223329\n",
            "epoch  84\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6887975774614339\n",
            "epoch  85\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6887546516768706\n",
            "epoch  86\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6887118867453687\n",
            "epoch  87\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6886692820462292\n",
            "epoch  88\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6886268369613153\n",
            "epoch  89\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6885845508750401\n",
            "epoch  90\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6885424231743549\n",
            "epoch  91\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6885004532487369\n",
            "epoch  92\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6884586404901771\n",
            "epoch  93\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6884169842931689\n",
            "epoch  94\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6883754840546958\n",
            "epoch  95\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6883341391742197\n",
            "epoch  96\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6882929490536696\n",
            "epoch  97\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.688251913097429\n",
            "epoch  98\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6882110307123256\n",
            "epoch  99\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6881703013076188\n",
            "epoch  100\n",
            "forward propadation\n",
            "backpropagation\n",
            "loss=  0.6881297242949892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD4CAYAAAD7CAEUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dd5hV1fn28e8zMwxIRxhAOsgADgoI\nI0gvggFpGgtib6goTdRE35gYTfwZY5QeFFTsXYOgIE2kgwwivYMUASnSpEh73j/OJpkgxBmcwylz\nf65rLjhr9jnzrGvr3Oy19l7L3B0REZGsSoh0ASIiElsUHCIiki0KDhERyRYFh4iIZIuCQ0REsiUp\n0gWcDSVKlPBKlSpFugwRkZgyb968He6ecnJ7rgiOSpUqkZGREekyRERiipmtP1W7hqpERCRbFBwi\nIpItCg4REckWBYeIiGSLgkNERLIlrMFhZm3NbIWZrTazR05zzHVmttTMlpjZ25nanzGzxcFXl0zt\nL5vZAjNbaGYfmlnBcPZBRET+W9iCw8wSgSFAOyAN6GpmaScdkwo8CjR295pAn6C9PVAXqAM0AB4y\ns8LB2x5w99ruXgvYAPQIVx9EROTnwnnFUR9Y7e5r3f0w8C7Q+aRjugFD3H0XgLtvC9rTgKnuftTd\n9wMLgbbBMXsBzMyAc4CwrQs/dtEWRs7/LlwfLyISk8IZHGWBjZlebwraMqsGVDOzGWY228zaBu0L\ngLZmlt/MSgAtgfIn3mRmI4CtQA1g0Kl+uJndbWYZZpaxffv2bBfv7nwwbxN93vuGxz9ZzOGjx7P9\nGSIi8SjSk+NJQCrQAugKDDezou4+HhgDzATeAWYBx068yd1vB8oAy4AunIK7D3P3dHdPT0n52RPz\nv8jMePHmetzVpDKvzVpPl2Gz2LLnYLY/R0Qk3oQzOL4j01UCUC5oy2wTMMrdj7j7OmAloSDB3Z9y\n9zru3gaw4Hv/5u7HCA1/XR2m+smTmMBjHdIYckNdVm7dR/uB05mxeke4fpyISEwIZ3DMBVLNrLKZ\nJQPXA6NOOmYkoasNgiGpasBaM0s0s+JBey2gFjDeQqoG7QZ0ApaHsQ8AtK91Hp/0aMy5BZK5+eU5\nDJm8muPHteWuiOROYQsOdz9K6I6ncYSGlN539yVm9qSZdQoOGwfsNLOlwGTgYXffCeQBpgXtw4Cb\ngs8z4DUzWwQsAs4DngxXHzKrWrIQn9zfmPa1yvDsuBXc/UYGew4cORs/WkQkqph7/P/LOT093XNq\ndVx357WZ3/LXz5ZxXtF8DL2xHheWLZIjny0iEk3MbJ67p5/cHunJ8ZhjZtzWuDLv3dOQI0ed3w6d\nyXtzN0S6LBGRs0bBcYbqVSzGZ72aUL/Sufz+o0U8/MECDh059stvFBGJcQqOX6F4wby8dkd9eraq\nygfzNnHVP2fy7Y79kS5LRCSsFBy/UmKC8eDl1Rlx+yVs2XOQjoOm8/nirZEuS0QkbBQcOaRl9ZJ8\n2rMJVVIKcO+b83jqs6UcOaanzUUk/ig4clC5Yvl5/96G3NKwIsOnraPrsNls3XMo0mWJiOQoBUcO\ny5uUyJOdL2TA9XVYumUv7QdOY/oqPW0uIvFDwREmneuUZVSPJhQvmMzNr8yh/8SVHNPT5iISBxQc\nYVS1ZEFG3t+Yq+qUpf/EVdw24it2/vhTpMsSEflVFBxhlj85ieeuq83Tv72IOet+oP3A6cz99odI\nlyUicsYUHGeBmdG1fgX+dV8j8uZJ4Pphs3lxyhpyw3IvIhJ/FBxnUc0yRRjdswmXp5Xi6bHL6fZ6\nBrsPHI50WSIi2aLgOMsK58vDP2+sy587pjFl5XbaD5zONxt3R7osEZEsU3BEwImFEj+4txEA174w\nkxEz1mnoSkRigoIjguqUL8pnvZrQvFoKT4xeSvc3v2bPQe3xISLRTcERYUXzJzP8lnT+cMUFTFz2\nPR0HTWfRpj2RLktE5LQUHFHAzOjWrArv3dOQo8eOc/XQmbw281sNXYlIVFJwRJHQHh9NaZJagsdH\nLeG+t75m7yENXYlIdFFwRJliBZJ56ZZ0Hm1Xg/FLv6fDwOks3KS7rkQkeig4olBCgnFP8/N5P9PQ\nle66EpFooeCIYvUqFmNM76b/vuvq3jfnseeAhq5EJLIUHFHuxF1Xj7W/gEnLttF+0DTmb9gV6bJE\nJBdTcMQAM+OuplX4sPuJBwZnMXzqWo5rmXYRiQAFRwypU74on/VsSusLSvHUmGXc9XoGP+zXWlci\ncnYpOGJMkfx5GHpTXZ7sXJPpq3ZwxYBpfLVOy7SLyNmj4IhBZsYtDSvx8X2NOCc5keuHzWLQpFXa\nYVBEzgoFRwy7sGxomfZOtcvw3ISV3PzyHLbtPRTpskQkzik4YlzBvEn061KHv19Ti/kbdtNuwDS+\nXLEt0mWJSBxTcMQBM+O69PKM7tmYEgXzctuIuTw9ZhlHjh2PdGkiEocUHHGkaslCfNKjMTc0qMCL\nU9dy7Quz2PjDgUiXJSJxRsERZ/LlSeT/rrqIITfUZc32H7liwDQ+Xbg50mWJSBwJa3CYWVszW2Fm\nq83skdMcc52ZLTWzJWb2dqb2Z8xscfDVJVP7W8FnLjazV8wsTzj7EKva1zqPMb2acn7JgvR4ez6P\nfryQg4ePRbosEYkDYQsOM0sEhgDtgDSgq5mlnXRMKvAo0NjdawJ9gvb2QF2gDtAAeMjMCgdvewuo\nAVwEnAPcFa4+xLry5+bng3sb0r3F+bw7dyOdBk9n+da9kS5LRGJcOK846gOr3X2tux8G3gU6n3RM\nN2CIu+8CcPcTtwOlAVPd/ai77wcWAm2DY8Z4APgKKBfGPsS8PIkJ/L5tDV6/oz67Dhyh0+AZvDF7\nvVbaFZEzFs7gKAtszPR6U9CWWTWgmpnNMLPZZtY2aF8AtDWz/GZWAmgJlM/8xmCI6mbg81P9cDO7\n28wyzCxj+/btOdCd2NY0NYWxvZvSsEpx/jhyMfe+OY/dB7RciYhkX6Qnx5OAVKAF0BUYbmZF3X08\nMAaYCbwDzAJOHqD/J6Grkmmn+mB3H+bu6e6enpKSEq76Y0pKobyMuO0S/nDFBXyxfJuWKxGRMxLO\n4PiO/75KKBe0ZbYJGOXuR9x9HbCSUJDg7k+5ex13bwNY8D0AzOxxIAXoG8b641JCQmh/84+6NyI5\nKYHrh82i34SVHNUzHyKSReEMjrlAqplVNrNk4Hpg1EnHjCR0tUEwJFUNWGtmiWZWPGivBdQCxgev\n7wJ+A3R1d/22O0O1yhXl015NufLisgyYtIquw2fz3e6DkS5LRGJA2ILD3Y8CPYBxwDLgfXdfYmZP\nmlmn4LBxwE4zWwpMBh52951AHmBa0D4MuCn4PIAXgFLALDP7xsz+FK4+xLuCeZN4/ro69OtSm6Wb\n99Ku/1TGLNoS6bJEJMpZbri7Jj093TMyMiJdRlRbv3M/vd79hgUbd3P9JeX5U8c08icnRbosEYkg\nM5vn7uknt0d6clyiRMXiBfjw3obc1+J83svYSIdB01n83Z5IlyUiUUjBIf+WJzGB37WtwVt3NmD/\nT0f57T9n8tI0bVErIv9NwSE/06hqCcb2bkbz6in89bNl3PbqXLbt0z4fIhKi4JBTOrdAMsNursdf\nrryQOWt30q7/NL5Y/n2kyxKRKKDgkNMyM26+tCKf9mxCSqG83PFqBo9/sphDR7RYokhupuCQX5Ra\nqhAj72/M7Y0r8dqs9XQePEOLJYrkYgoOyZJ8eRJ5vGNNXr39EnbuP0ynwTMYMWOdFksUyYUUHJIt\nLaqX5PM+TWlStQRPjF7KbSPmsn3fT5EuS0TOIgWHZFuJgnl5+dZ0nuxck9lrd9K2/1RNnIvkIgoO\nOSNmxi0NKzE608T5H0cu1i6DIrmAgkN+lWqlCvFJj8bc1aQyb8xeT8fB01myWU+ci8QzBYf8anmT\nEnmsQxpv3FmffYeOcOWQGbw4ZY2eOBeJUwoOyTFNU1P4vHczLqtRiqfHLufGl+awWUu1i8QdBYfk\nqGIFkhl6U13+fnUtFmzaTdv+Uxm9YHOkyxKRHKTgkBxnZlx3SXnG9GpKlZSC9HxnPg+89w17Dx2J\ndGkikgMUHBI2lUqElmrv0zqVUQs2067/NOas3RnpskTkV1JwSFglJSbQp3U1Pri3IUmJxvXDZ/PM\n58s5fFS7/orEKgWHnBV1KxRjTK+mdEkvz9Av13DVP2ew6vt9kS5LRM6AgkPOmgJ5k/jb1bUYdnM9\ntuw5RIdB03l1xjrdtisSYxQcctZdXrM0n/dpSqPzi/Pn0Uu5dcRXbN2jjaJEYoWCQyKiZKF8vHLb\nJfz1ygvJ+HYXv+k/lU8X6rZdkVig4JCIMTNuurQin/VqQqUSBejx9nx6vzufPQd0265INFNwSMRV\nSSnIR8Ftu58u3ELbAVOZsXpHpMsSkdNQcEhUOHHb7sfdG3FOciI3vjSHJ0Yv0Ta1IlFIwSFRpXb5\nonzWsym3NqzIiBnf0mHQdBZ/p9V2RaKJgkOizjnJiTzR+UJev+M/q+0OmrSKo8f00KBINFBwSNRq\nVi2F8X2ac8VF5/HchJVc88Is1m7/MdJlieR6Cg6JakXy52Fg14sZ2PVi1u3YzxUDp/HazG/10KBI\nBCk4JCZ0ql2G8Q8049IqxXl81BJueeUr7fUhEiEKDokZpQrnY8Rtl/DUVRfy9YbQQ4Mff70Jd119\niJxNCg6JKWbGjQ0qMrZ3U6qXKkTf9xfQ/c2v2fnjT5EuTSTXUHBITKpYvADv3dOQR9rV4Ivl27i8\n31TGLdka6bJEcoWwBoeZtTWzFWa22sweOc0x15nZUjNbYmZvZ2p/xswWB19dMrX3CD7PzaxEOOuX\n6JaYYNzb/HxG92xC6SL5uOeNefR97xv2HNSSJSLhFLbgMLNEYAjQDkgDuppZ2knHpAKPAo3dvSbQ\nJ2hvD9QF6gANgIfMrHDwthlAa2B9uGqX2FK9dCH+dV9jerWqyicLNtO2/1Smrdoe6bJE4lY4rzjq\nA6vdfa27HwbeBTqfdEw3YIi77wJw921Bexow1d2Puvt+YCHQNjhmvrt/G8a6JQYlJyXQ9/Lq/Ou+\nRhTIm8TNL3/FYyMXsf+no5EuTSTuhDM4ygIbM73eFLRlVg2oZmYzzGy2mbUN2hcAbc0sfzAc1RIo\nn50fbmZ3m1mGmWVs365/feYWtcoV5dOeTejWtDJvzdlAuwHa51wkp0V6cjwJSAVaAF2B4WZW1N3H\nA2OAmcA7wCwgW6vdufswd0939/SUlJScrVqiWr48ifyhfRrv39MQgOuHz+Yvny7VgokiOSScwfEd\n/32VUC5oy2wTMMrdj7j7OmAloSDB3Z9y9zru3gaw4HsiWXZJpXMZ27spNzWoyMvT13HFwGnM37Ar\n0mWJxLxwBsdcINXMKptZMnA9MOqkY0YSutogGJKqBqw1s0QzKx601wJqAePDWKvEqQJ5k/jLlRfy\n5p0N+OnIca4eOpNnPl/OT0d19SFypsIWHO5+FOgBjAOWAe+7+xIze9LMOgWHjQN2mtlSYDLwsLvv\nBPIA04L2YcBNwedhZr3MbBOhK5iFZvZSuPog8aNJagk+79OUa+uVZ+iXa+g4aDoLN+2OdFkiMcly\nw3IN6enpnpGREekyJEpMXrGNRz5ayI4fD3Nfi/Pp2SqV5KRIT/eJRB8zm+fu6Se36/8WyXVaVi/J\n+D7N6VynDIO+WE2nwdosSiQ7FBySKxXJn4fnr6vDS7eks3P/Ya4cMoN+E1Zy+Kg2ixL5JQoOydVa\np5ViwgPN6FDrPAZMWkXnITNYunlvpMsSiWoKDsn1iuZPpv/1FzPs5nps3/cTnQZPp/9EXX2InI6C\nQyRwec3STOwbuvroPzF09bFks+Y+RE6m4BDJJPPVx44ff6LzYM19iJwsS8FhZr3NrLCFvGxmX5vZ\n5eEuTiRSLq9ZmgkPNKNj7TIMmLRKd16JZJLVK4473H0vcDlQDLgZ+FvYqhKJAkXzJ9OvS+jOqx/2\nH6bzkBk8N36FnjqXXC+rwWHBn1cAb7j7kkxtInEtdOfVf5776DhoOgs26qlzyb2yGhzzzGw8oeAY\nZ2aFAA36Sq5x4rmPV25LZ+/Bo1z1zxn8bexyrbgruVJWg+NO4BHgEnc/QGgtqdvDVpVIlGpVoxTj\nHmjGtfXK88KUNVwxcBrz1v8Q6bJEzqqsBkdDYIW77zazm4DHAM0USq5U5Jw8PHNNLV6/oz4/HTnO\nNS/M4i+fLuXgYV19SO6Q1eAYChwws9rAg8Aa4PWwVSUSA5pVS2HcA824sUEFXp6+jrYDpjJrjXYb\nlPiX1eA46qFldDsDg919CFAofGWJxIaCeZP465UX8e7dlwLQdfhs/vCvRew7dCTClYmET1aDY5+Z\nPUroNtzPzCyB0DyHiACXVinO572bcVeTyrz91QZ+028qk1dsi3RZImGR1eDoAvxE6HmOrYQ2UXo2\nbFWJxKBzkhN5rEMaH3VvRP68Sdw+Yi593/uGXfsPR7o0kRyVpeAIwuItoIiZdQAOubvmOEROoW6F\nYnzWqwk9W1Vl1ILNtOk3hTGLtkS6LJEck9UlR64DvgKuBa4D5pjZNeEsTCSW5U1K5MHLq/NJj8aU\nLpKP+976mnvfmMe2vYciXZrIr5alrWPNbAHQxt23Ba9TgInuXjvM9eUIbR0rkXT02HGGT1tHv4kr\nyZeUwGMd0ri2XjnMtPiCRLdfu3VswonQCOzMxntFcrWkxAS6tzifsb2bUqN0YX734UJufvkrNv5w\nINKliZyRrP7y/9zMxpnZbWZ2G/AZMCZ8ZYnEn/NTCvLu3Zfy1ysv5JuNu7m831Renr6OY8d/+apf\nJJpkaagKwMyuBhoHL6e5+7/CVlUO01CVRJvNuw/y2MjFfLF8G3XKF+WZq2tRvbQejZLocrqhqiwH\nRyxTcEg0cndGLdjME6OXsu/QEbq3qMr9Lc8nb1JipEsTAc5wjsPM9pnZ3lN87TOzveErVyT+mRmd\n65RlwgPN6FCrDAMnraL9wOlaNFGi3v8MDncv5O6FT/FVyN0Ln60iReJZ8YJ56delDiNuv4SDh49x\nzQuz+NMni7VsiUQt3RklEiVaVi/J+AeacVujSrwxez2X95vKpGXfR7oskZ9RcIhEkQJ5k3i8Y00+\n7t6IwvnycOdrGdz/9tds3/dTpEsT+TcFh0gUurhCMUb3bMJDl1djwpLvaf38FN6fu5HccDOLRD8F\nh0iUSk5KoEerVMb2aUr1UoX43UcL6Tp8Nut27I90aZLLKThEotyJBwf/76qLWLJ5L7/pP5Uhk1dz\n+OjxSJcmuZSCQyQGJCQYNzSowKS+zbmsRkmeHbeCjoOm8/WGXZEuTXIhBYdIDClZOB9Db6rH8FvS\n2XvoCFcPncnjunVXzrKwBoeZtTWzFWa22sweOc0x15nZUjNbYmZvZ2p/xswWB19dMrVXNrM5wWe+\nZ2bJ4eyDSDRqk1aKCX2bc2vDSrw+ez1tnp/KuCVbI12W5BJhCw4zSwSGAO2ANKCrmaWddEwq8CjQ\n2N1rAn2C9vZAXaAO0AB4yMxOPHD4DNDP3asCu4A7w9UHkWhWMG8Sf+5Uk3/d15ii+fNwzxvzuPv1\nDLbsORjp0iTOhfOKoz6w2t3Xuvth4F2g80nHdAOGuPsugExLt6cBU939qLvvBxYCbS20gUEr4MPg\nuNeAK8PYB5GoV6d8UUb3bMLv29Zg6qrttHl+Kq/O0Kq7Ej7hDI6ywMZMrzcFbZlVA6qZ2Qwzm21m\nbYP2BYSCIr+ZlQBaAuWB4sBudz/6Pz4TADO728wyzCxj+/btOdQlkeiUJ9jzY3yf5lxcoSh/Hr2U\n3w6dydLNWlJOcl6kJ8eTgFSgBdAVGG5mRd19PKH9PmYC7wCzgGPZ+WB3H+bu6e6enpKSkrNVi0Sp\nCsXz8/od9RlwfR2+23WAjoOn8/SYZRw4fPSX3yySReEMju8IXSWcUC5oy2wTMMrdj7j7OmAloSDB\n3Z9y9zru3gaw4Hs7gaJmlvQ/PlMkVzux6u7Evs25tl45Xpy6ljbPT2Xy8m2//GaRLAhncMwFUoO7\noJKB64FRJx0zktDVBsGQVDVgrZklmlnxoL0WUAsY76H1FiYD1wTvvxX4JIx9EIlZRfMn87era/H+\nPQ05JzmR21+dy/1vfc33ew9FujSJcWELjmAeogcwDlgGvO/uS8zsSTPrFBw2DthpZksJBcLD7r4T\nyANMC9qHATdlmtf4PdDXzFYTmvN4OVx9EIkH9Sufy5heTUPrXi37ntbPTeG1md9q8lzOmHYAFMlF\nvt2xnz9+sphpq3ZQu1wRnrrqIi4sWyTSZUmUOqMdAEUkvlQqUeA/k+e7D9Jp8HSeHL2UH3/S5Llk\nnYJDJJc5MXk+qW8LutavwIiZ62j93BQ+X7xFy7ZLlig4RHKpIvnz8NRVF/FR90YUK5DMvW9+zV2v\nZbDxhwORLk2inIJDJJerW6EYo3s05g9XXMCstTtp028KQ79cw5FjWrZdTk3BISIkJSbQrVkVJvZt\nTrPUFJ75fDlXDJjGnLU7I12aRCEFh4j8W5mi5zDslnReuiWdA4eP0WXYbB76YAE7f9Se5/IfCg4R\n+ZnWaaWY0LcZ9zY/n5Hzv6PVc1N456sNHNezH4KCQ0ROI39yEo+0q8GY3k2pXroQj368iKtfmMmS\nzXsiXZpEmIJDRP6naqUK8d7dl/LctbXZsPMAHQdN54nRS7TrYC6m4BCRX2RmXF2vHF88GHr249WZ\n33LZc1MYtWCznv3IhRQcIpJlJ579GHlfY0oWzkuvd+Zz08tzWLP9x0iXJmeRgkNEsq12+aJ8cn8T\nnuxck4Wb9tC2/1SeHbecg4eztW2OxCgFh4ickcQE45aGlfjiwRZ0qFWGIZPX0Pr5KUxY+n2kS5Mw\nU3CIyK+SUigv/brU4d27L6VA3kS6vZ7Bna/OZcNOLV0SrxQcIpIjLq1SnM96NeX/XVGD2cHSJQMm\nruLQEQ1fxRsFh4jkmDyJCdzd7HwmPdiCNmml6DdxJZf3m8oXyzV8FU8UHCKS40oXycfgG+ry1l0N\nyJNo3PFqhlbejSMKDhEJm8ZVSzC2dzMeaVeDGat30Pr5KQycpOGrWKfgEJGwSk5K4N7m5zPpwea0\nvqAUz09YyW/6a/gqlik4ROSsKFP0HIbcWJc372xAYsKJ4SvdfRWLFBwiclY1SS3B572b8Wi7Gsxc\ns5PW/abw/ISVGr6KIQoOETnrkpMSuKf5+XzxYAt+U7M0AyetovXzUxi3ZKvWvooBCg4RiZjSRfIx\nqOvFvNPtUvInJ3LPG/O4dcRcrX0V5RQcIhJxDc8PPTz4pw5pzF+/i7b9p/L02GX8+NPRSJcmp6Dg\nEJGokCcxgTuaVOaLh1rQuU5ZXpyylsue+5KR87/T8FWUUXCISFRJKZSXf1xbm4+6N6JkoXz0ee8b\nrntxlnYejCIKDhGJSvUqFmPk/Y15+rcXsWb7fjoOms5jIxexa//hSJeW6yk4RCRqJSYYXetXYPKD\nLbilYSXenrOBls99yRuz13PsuIavIkXBISJRr0j+PPy5U03G9G5KjdKF+OPIxXQYNJ2v1v0Q6dJy\nJQWHiMSMGqUL8063Sxl8w8XsOXCY616cRc935rNlz8FIl5arKDhEJKaYGR1qlWHSgy3odVkq45ds\npdU/pjBIiyeeNWENDjNra2YrzGy1mT1ymmOuM7OlZrbEzN7O1P73oG2ZmQ00Mwvau5jZwuB7z4Sz\nfhGJXuckJ9K3TTUm9m1Oi+opPDdhJa2fn8Lni7fo9t0wC1twmFkiMARoB6QBXc0s7aRjUoFHgcbu\nXhPoE7Q3AhoDtYALgUuA5mZWHHgWuCw4vrSZXRauPohI9Ct/bn6G3lSPt+9qQIHkJO5982tufGkO\nK7bui3RpcSucVxz1gdXuvtbdDwPvAp1POqYbMMTddwG4+7ag3YF8QDKQF8gDfA9UAVa5+/bguInA\n1WHsg4jEiEZVS/BZryY82bkmSzbv5YqB03j8k8XsPqDbd3NaOIOjLLAx0+tNQVtm1YBqZjbDzGab\nWVsAd58FTAa2BF/j3H0ZsBqobmaVzCwJuBIoH8Y+iEgMSUpM4JaGlfjyoRbcUL8Cb8xeT4t/fMnr\ns77l6LHjkS4vbkR6cjwJSAVaAF2B4WZW1MyqAhcA5QiFTSszaxpcmXQH3gOmAd8Cp5wNM7O7zSzD\nzDK2b99+qkNEJE4VK5DMX668kDG9m3JB6cL86ZMlXDFwGjNW74h0aXEhnMHxHf99NVAuaMtsEzDK\n3Y+4+zpgJaEguQqY7e4/uvuPwFigIYC7j3b3Bu7eEFgRvOdn3H2Yu6e7e3pKSkqOdkxEYkON0oV5\nu1sDXripLgePHOPGl+bQ7fUM1u/cH+nSYlo4g2MukGpmlc0sGbgeGHXSMSMJXW1gZiUIDV2tBTYQ\nmgxPMrM8QHNgWXBcyeDPYsB9wEth7IOIxDgzo+2F5zHhgeY8/JvqzFi9gzbPh1bf3XfoSKTLi0lh\nCw53Pwr0AMYR+qX/vrsvMbMnzaxTcNg4YKeZLSU0p/Gwu+8EPgTWAIuABcACdx8dvGdAcPwM4G/u\nfsorDhGRzPLlSeT+llX58qEWdKpThhenrKXlP6bw3twNWr4kmyw33O+cnp7uGRkZkS5DRKLIwk27\neWL0Uuat30XNMoX5U4c0GlQpHumyooqZzXP39JPbIz05LiISEbXKFeXDexsysOvF7Np/mC7DZtP9\nzXls/OFApEuLekmRLkBEJFLMjE61y9DmglIMm7qWF6asYdKybdzZtDL3t6xKwbz6FXkquuIQkVzv\nnOREerdOZfJDLehQ+zyGfrmGFs9+ybtfaf7jVBQcIiKB0kXy8fx1dRh5f2MqFs/PIx8vosOg6cxc\no+c/MlNwiIicpE750PzH4BsuZu/BI9wwPPT8x7odev4DFBwiIqf0n+XbQ89/zFy9gzbPT+HJ0UvZ\ncyB3P/+h4BAR+R/+/fzHwy25Nr0cr85cR/N/TGbEjHUcyaXrXyk4RESyIKVQXp7+bS0+69WUmmUK\n88Topfym/1QmLv0+1+3/oeAQEcmGC84rzJt3NuCV20LPxd31egY3vjSHJZv3RLiys0fBISKSTWZG\nqxqlGNenGU90qsmyLXvpMA0lWcEAAAiaSURBVGg6D3+wgO/3Hop0eWGn4BAROUN5EhO4tVElvny4\nJXc1qcwn32ymxbNf0m/CSg4cPhrp8sJGwSEi8isVOScPf2ifxsS+zWlVoyQDJq2ixbNf8v7cjXH5\nAKGCQ0Qkh1Qonp8hN9blo+4NKVvsHH730ULaD5zGtFXxtZmcgkNEJIfVq3guH3dvxOAbLubHn45y\n88tfcesrX7Fi675Il5YjFBwiImGQ+QHCP1xxAfM37KLdgKk88tFCtsX4BLr24xAROQt27T/MoC9W\n88bsb8mTmMDdzarQrWkVCkTxCrzaj0NEJIKKFUjmTx1DE+gtq5ek/8RVtPjHl7zz1QaOxtgT6AoO\nEZGzqGLxAsEEeiMqnJufRz9exBUDp/HF8th5Al3BISISAfUqFuPDexsy9Ma6HD56nDtezeCG4XNY\ntCn6n0BXcIiIRIiZ0e6i8xj/QHOe6FSTFd/vo+Pg6fR+d35Ub2GryXERkSix79ARXpiyhpenr+P4\ncbilYUV6tKpK0fzJEanndJPjCg4RkSizZc9B+k1YyQfzNlEobxL3t6zKrY0qkS9P4lmtQ3dViYjE\niPOKnMPfr6nN2N5NqVexGE+PXU6rf3zJR/M2RcUSJgoOEZEoVaN0YUbcXp+3uzWgRKG8PPjBAtoP\nnMbkFdsiegeWgkNEJMo1Or8EI+9rzKCuF3Pg8DFuHzGXG1+aw8JNuyNSj4JDRCQGJCQYHWuXYWLf\n5jzeMY3lW/fRafAMerz9Net37j+rtWhyXEQkBu07dIRhU9fy0rTQ3uc3NqhAz8tSKVEwb479DN1V\npeAQkTi0be8hBkxaxbtzN5IvKYG7mlahW7MqFMyBNbAUHAoOEYlja7f/yLPjVjB28VaKF0imZ6uq\n3NCgIslJZz4jodtxRUTiWJWUggy9qR7/uq8RqaUK8ufRS2n9/JSw7AGi4BARiSMXVyjGO90u5dXb\nL6Fi8fyUP/ecHP8Z0bsQvIiInBEzo0X1krSoXjIsnx/WKw4za2tmK8xstZk9cppjrjOzpWa2xMze\nztT+96BtmZkNNDML2rua2SIzW2hmn5tZiXD2QURE/lvYgsPMEoEhQDsgDehqZmknHZMKPAo0dvea\nQJ+gvRHQGKgFXAhcAjQ3syRgANDS3WsBC4Ee4eqDiIj8XDivOOoDq919rbsfBt4FOp90TDdgiLvv\nAnD3bUG7A/mAZCAvkAf4HrDgq0BwBVIY2BzGPoiIyEnCGRxlgY2ZXm8K2jKrBlQzsxlmNtvM2gK4\n+yxgMrAl+Brn7svc/QjQHVhEKDDSgJdP9cPN7G4zyzCzjO3bt+dkv0REcrVI31WVBKQCLYCuwHAz\nK2pmVYELgHKEwqaVmTU1szyEguNioAyhoapHT/XB7j7M3dPdPT0lJSX8PRERySXCeVfVd0D5TK/L\nBW2ZbQLmBFcS68xsJf8Jktnu/iOAmY0FGgKHANx9TdD+PnDKSXcREQmPcF5xzAVSzayymSUD1wOj\nTjpmJKGQILg7qhqwFthAMBkeXGU0B5YRCp40MztxCdEmaBcRkbMkbFcc7n7UzHoA44BE4BV3X2Jm\nTwIZ7j4q+N7lZrYUOAY87O47zexDoBWhuQwHPnf30QBm9gQw1cyOAOuB28LVBxER+blcsVaVmW0n\nFDJnogSwIwfLiRW5sd+5sc+QO/utPmdNRXf/2SRxrgiOX8PMMk61yFe8y439zo19htzZb/X514n0\nXVUiIhJjFBwiIpItCo5fNizSBURIbux3buwz5M5+q8+/guY4REQkW3TFISIi2aLgEBGRbFFw/A9Z\n2U8k1plZeTObnGlPlN5B+7lmNsHMVgV/Fot0rTnNzBLNbL6ZfRq8rmxmc4Lz/V6w4kFcCdaC+9DM\nlgd73TSM93NtZg8E/20vNrN3zCxfPJ5rM3vFzLaZ2eJMbac8txYyMOj/QjOrm52fpeA4jazsJxIn\njgIPunsacClwf9DPR4BJ7p4KTCI+1wTrzX8vWfMM0M/dqwK7gDsjUlV4DSC0EkMNoDah/sftuTaz\nskAvIN3dLyS0isX1xOe5fhVoe1Lb6c5tO0LrAqYCdwNDs/ODFBynl5X9RGKeu29x96+Dv+8j9Iuk\nLKG+vhYc9hpwZWQqDA8zKwe0B14KXhuhZW4+DA6Jxz4XAZoRbEXg7ofdfTdxfq4JLa10TrARXH5C\nWzXE3bl296nADyc1n+7cdgZe95DZQFEzOy+rP0vBcXpZ2U8krphZJUJL1s8BSrn7luBbW4FSESor\nXPoDvwOOB6+LA7vd/WjwOh7Pd2VgOzAiGKJ7ycwKEMfn2t2/A/5BaOHULcAeYB7xf65PON25/VW/\n3xQcAoCZFQQ+Avq4+97M3/PQPdtxc9+2mXUAtrn7vEjXcpYlAXWBoe5+MbCfk4al4vBcFyP0r+vK\nhPbwKcDPh3NyhZw8twqO08vKfiJxIVi6/iPgLXf/OGj+/sSla/DnttO9PwY1BjqZ2beEhiBbERr7\nLxoMZ0B8nu9NwCZ3nxO8/pBQkMTzuW4NrHP37cG+Px8TOv/xfq5PON25/VW/3xQcp5eV/URiXjC2\n/zKwzN2fz/StUcCtwd9vBT4527WFi7s/6u7l3L0SofP6hbvfSGi74muCw+KqzwDuvhXYaGbVg6bL\ngKXE8bkmNER1qZnlD/5bP9HnuD7XmZzu3I4CbgnurroU2JNpSOsX6cnx/8HMriA0Fn5iP5GnIlxS\njjOzJsA0QnufnBjv/3+E5jneByoQWpL+Onc/eeIt5plZC+Ahd+9gZlUIXYGcC8wHbnL3nyJZX04z\nszqEbghIJrRp2u2E/gEZt+c62MOnC6E7COcDdxEaz4+rc21m7xDaGK8E8D3wOKHN8n52boMQHUxo\n2O4AcLu7Z2T5Zyk4REQkOzRUJSIi2aLgEBGRbFFwiIhItig4REQkWxQcIiKSLQoOERHJFgWHiIhk\ny/8H7lsuME3WBssAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.6929883926572735,\n",
              "  0.6929296703511896,\n",
              "  0.6928711707285004,\n",
              "  0.6928128929048596,\n",
              "  0.6927548359998131,\n",
              "  0.6926969991367771,\n",
              "  0.6926393814430183,\n",
              "  0.6925819820496335,\n",
              "  0.6925248000915294,\n",
              "  0.6924678347074018,\n",
              "  0.6924110850397163,\n",
              "  0.6923545502346882,\n",
              "  0.6922982294422623,\n",
              "  0.6922421218160943,\n",
              "  0.6921862265135309,\n",
              "  0.6921305426955895,\n",
              "  0.6920750695269404,\n",
              "  0.6920198061758872,\n",
              "  0.6919647518143476,\n",
              "  0.6919099056178348,\n",
              "  0.691855266765439,\n",
              "  0.6918008344398082,\n",
              "  0.6917466078271308,\n",
              "  0.6916925861171161,\n",
              "  0.6916387685029768,\n",
              "  0.6915851541814111,\n",
              "  0.6915317423525836,\n",
              "  0.6914785322201095,\n",
              "  0.6914255229910344,\n",
              "  0.691372713875819,\n",
              "  0.6913201040883195,\n",
              "  0.6912676928457719,\n",
              "  0.6912154793687744,\n",
              "  0.6911634628812688,\n",
              "  0.6911116426105259,\n",
              "  0.6910600177871266,\n",
              "  0.6910085876449453,\n",
              "  0.6909573514211348,\n",
              "  0.6909063083561073,\n",
              "  0.69085545769352,\n",
              "  0.6908047986802575,\n",
              "  0.6907543305664156,\n",
              "  0.6907040526052857,\n",
              "  0.6906539640533382,\n",
              "  0.6906040641702071,\n",
              "  0.6905543522186729,\n",
              "  0.6905048274646484,\n",
              "  0.6904554891771617,\n",
              "  0.6904063366283417,\n",
              "  0.6903573690934017,\n",
              "  0.6903085858506248,\n",
              "  0.6902599861813478,\n",
              "  0.6902115693699468,\n",
              "  0.6901633347038216,\n",
              "  0.690115281473381,\n",
              "  0.6900674089720273,\n",
              "  0.6900197164961428,\n",
              "  0.6899722033450735,\n",
              "  0.6899248688211155,\n",
              "  0.6898777122294997,\n",
              "  0.6898307328783788,\n",
              "  0.6897839300788108,\n",
              "  0.6897373031447467,\n",
              "  0.6896908513930151,\n",
              "  0.6896445741433086,\n",
              "  0.6895984707181699,\n",
              "  0.6895525404429772,\n",
              "  0.6895067826459312,\n",
              "  0.6894611966580411,\n",
              "  0.6894157818131104,\n",
              "  0.6893705374477239,\n",
              "  0.6893254629012342,\n",
              "  0.689280557515748,\n",
              "  0.6892358206361128,\n",
              "  0.6891912516099037,\n",
              "  0.6891468497874101,\n",
              "  0.6891026145216232,\n",
              "  0.6890585451682221,\n",
              "  0.6890146410855613,\n",
              "  0.6889709016346581,\n",
              "  0.688927326179179,\n",
              "  0.6888839140854278,\n",
              "  0.6888406647223329,\n",
              "  0.6887975774614339,\n",
              "  0.6887546516768706,\n",
              "  0.6887118867453687,\n",
              "  0.6886692820462292,\n",
              "  0.6886268369613153,\n",
              "  0.6885845508750401,\n",
              "  0.6885424231743549,\n",
              "  0.6885004532487369,\n",
              "  0.6884586404901771,\n",
              "  0.6884169842931689,\n",
              "  0.6883754840546958,\n",
              "  0.6883341391742197,\n",
              "  0.6882929490536696,\n",
              "  0.688251913097429,\n",
              "  0.6882110307123256,\n",
              "  0.6881703013076188,\n",
              "  0.6881297242949892],\n",
              " array([array([[0.00389529, 0.00721483, 0.0097272 ],\n",
              "        [0.00797368, 0.00362593, 0.00232007],\n",
              "        [0.00844308, 0.00793131, 0.00476528],\n",
              "        [0.00492605, 0.00763006, 0.00886512]]),\n",
              "        array([[0.00752349, 0.00911486, 0.00820708, 0.00368839],\n",
              "        [0.00966602, 0.00109326, 0.00239772, 0.00452318],\n",
              "        [0.00447285, 0.00806798, 0.00421526, 0.0050127 ]]),\n",
              "        array([[ 0.06170959,  0.05710208,  0.05756793],\n",
              "        [-0.0001748 ,  0.00314403,  0.00810365]])], dtype=object))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtjQw_JPSFDD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dae9257d-2d8f-4a56-9970-9acad97324c7"
      },
      "source": [
        "nn.dW[1].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68ZtH0XXSovm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3bd7f36-3fdb-4df1-b946-d5d997da76fb"
      },
      "source": [
        "nn.weights[1].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tzNGnIByZ1b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1437a93c-be64-4e9c-ec86-c306aa4d151b"
      },
      "source": [
        "a =np.array([[1,2,3],[4,2,5]])\n",
        "b =np.array([[-1,-2,1],[4,-3,1],[1,2,9]])\n",
        "c=a.dot(b)\n",
        "c"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10, -2, 30],\n",
              "       [ 9, -4, 51]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwZrHURyQ3w1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "379a2a9a-b8f4-44bd-9f31-87a08de11356"
      },
      "source": [
        "w=np.array([[-1,-2],[4,-3],[1,2]])\n",
        "w.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43qTVPzjRKNn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ac7fa56b-e3c2-49f7-a7e5-7d2c1a5b6902"
      },
      "source": [
        "a=[]\n",
        "X=np.array([[1,2],[5,6]])\n",
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2],\n",
              "       [5, 6]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwQKhKuRSMh2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "083a2d29-5e94-4e43-86c1-43d276c89207"
      },
      "source": [
        "def sigmoid(z):\n",
        "  s= 1/(1 + np.exp(-z)) \n",
        "  return s\n",
        "def Relu(z):\n",
        "  return np.maximum(0,z)\n",
        "\n",
        "def tanh(z):\n",
        "  return (np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z))\n",
        "\n",
        "def softmax(y):\n",
        "  exps = np.exp(y)\n",
        "  return exps / np.sum(exps)\n",
        "\n",
        "def Activation(z,method='sigmoid'):\n",
        "  if (method=='sigmoid'):\n",
        "    print('activation =sigmoid')\n",
        "    return sigmoid(z)\n",
        "  elif (method=='Relu'):\n",
        "    return Relu(z)\n",
        "\n",
        "z=np.dot(w , X).T\n",
        "layer_output =Activation(z,'sigmoid') \n",
        "a.append(layer_output)\n",
        "print(z)\n",
        "print(np.sum(z,axis=0,keepdims=True).T.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "activation =sigmoid\n",
            "[[-11 -11  11]\n",
            " [-14 -10  14]]\n",
            "(3, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpchiX7w65XK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "81097691-f12d-4a82-eecb-c40fee5c0ba3"
      },
      "source": [
        "for i in range(4):\n",
        "  \n",
        "  print (i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGMv6tJ7gEaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic={2:'sigmoid',3:\"Relu\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGT3bquAgMO1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf54472c-61fb-4fac-fcfd-4868dd370633"
      },
      "source": [
        "dic.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys([2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hymGTWs_YIc8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "551d2726-dd58-45b2-aacf-d9996c54bac6"
      },
      "source": [
        "net=neural_net(np.array([2,4,5]),{4:'sigmoid',3:'sigmoid',2:'Relu'})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-6032d16a306d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneural_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Relu'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-8a98d3f5300f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input, nodes_per_layer)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes_per_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnodes_per_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlayer_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes_per_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes_per_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m#[4,3,2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI5hSstbaYRL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71cc6718-852d-42f8-dfa9-bc38d718afe6"
      },
      "source": [
        "net.input"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 4, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ7sSCMWF7ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input=np.array([[1,2,3],[1,2,3]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ-JuUFfGFvp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "e073ab66-11c6-4466-8207-c80eda041da0"
      },
      "source": [
        "input.size[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e2790f6ce319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh7R2fgoGL6i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8f3f3b4-0558-4226-8b25-189c264f5181"
      },
      "source": [
        "input.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9Haq52VGPGy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "65b69bfe-32f4-451d-ad47-afec07d681d4"
      },
      "source": [
        "for i in input:\n",
        "  print (i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9GSGjq1G1MX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
