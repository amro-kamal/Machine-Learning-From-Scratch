{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Softmax derivative.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyGbXz1umGf8"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCHJoe0SlxPz"
      },
      "source": [
        "class Softmax(object):\n",
        "  \"\"\"\n",
        "  \n",
        "  \"\"\"\n",
        "  @staticmethod\n",
        "  def forward(logits):\n",
        "      \"\"\"\n",
        "      Computes the forward pass for a Softmax layer.\n",
        "      Input:\n",
        "      - x: Input; a tensor of any shape\n",
        "      Returns a tuple of:\n",
        "      - out: Output, a tensor of the same shape as x\n",
        "      - cache: x\n",
        "      \"\"\"\n",
        "\n",
        "      # out = x * torch.gt(x, 0)\n",
        "      # cache = x\n",
        "      x_max=torch.max(logits)\n",
        "      exp=torch.exp(logits-x_max)\n",
        "      return exp/torch.sum(exp , axis=1)\n",
        "  @staticmethod\n",
        "  def backward(dout, cache ,y):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a Softmax layer.\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: Input x, of same shape as dout\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    s = cache\n",
        "    # dlocal = s * (1-s) * dout  \n",
        "    dlocal =torch.min(dout) * s * (1-s) * y - torch.min(dout) * torch.max(s*y) * s * (1-y)\n",
        "  \n",
        "    return dlocal"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_a_6YUJmIV8"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "input  = torch.rand((10,1) ) # the input to the network\n",
        "w = torch.rand((3,10) , requires_grad=True) # the weights tensor of shape (3x10)\n",
        "y=torch.tensor([0,1,0]) # The ground truth output\n",
        "\n",
        "linear = torch.mm(w,input) # (3x1) logits before the softmax"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HXA5dh2zZXj"
      },
      "source": [
        "s=Softmax.forward(linear.reshape(1,-1)) # (3x1) the probabilities after the softmax\n",
        "dl_ds = -y / s   #upstream gradient \n",
        "dl_dlinear = Softmax.backward(dl_ds ,  s , y) # local gradient\n",
        "dl_dw = dl_dlinear * input # the drivative of the q4 with recpect to the weights\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAi8KlDQzb5S",
        "outputId": "1ef19581-7d98-40b6-a187-5176be094387"
      },
      "source": [
        "dl_dw.t() # the gradient of the loss with recpect to the weights"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1269,  0.1964,  0.0226,  0.0337,  0.0786,  0.1621,  0.1253,  0.2291,\n",
              "          0.1165,  0.1616],\n",
              "        [-0.3325, -0.5148, -0.0593, -0.0885, -0.2060, -0.4249, -0.3284, -0.6007,\n",
              "         -0.3053, -0.4237],\n",
              "        [ 0.2057,  0.3184,  0.0367,  0.0547,  0.1274,  0.2628,  0.2031,  0.3715,\n",
              "          0.1888,  0.2621]], grad_fn=<TBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byxdr21_zkKO"
      },
      "source": [
        "lets compare the result to pytorch backward() function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1JK6WHqzesO",
        "outputId": "7f816573-a45c-4db5-b361-2b4206e0cf10"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "input  = torch.rand((10,1) ) # the input to the network\n",
        "w = torch.rand((3,10) , requires_grad=True) # the weights tensor of shape (3x10)\n",
        "y=torch.tensor([0,1,0]) # The ground truth output\n",
        "\n",
        "linear = torch.mm(w,input) # (3x1) logits before the softmax\n",
        "softmax =nn.Softmax()\n",
        "soft = softmax(linear.reshape(1,-1))\n",
        "\n",
        "loss = torch.sum( -y * torch.log(soft) ) # (scalar) the cross entropy loss between s and y \n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ot9z3MyXzoa1"
      },
      "source": [
        "loss.backward() # compute the gardient using autograd engine"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPuuOW5jzq8R",
        "outputId": "ea9e746e-1e9d-49b8-b1d2-54f886c5c5e5"
      },
      "source": [
        "w.grad"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1269,  0.1964,  0.0226,  0.0337,  0.0786,  0.1621,  0.1253,  0.2291,\n",
              "          0.1165,  0.1616],\n",
              "        [-0.3325, -0.5148, -0.0593, -0.0885, -0.2060, -0.4249, -0.3284, -0.6007,\n",
              "         -0.3053, -0.4237],\n",
              "        [ 0.2057,  0.3184,  0.0367,  0.0547,  0.1274,  0.2628,  0.2031,  0.3715,\n",
              "          0.1888,  0.2621]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36MRxx7uz6Sz",
        "outputId": "37c6946d-097b-43d0-b4c8-07dec6a848ad"
      },
      "source": [
        "if torch.sum(dl_dw.t() - w.grad)< 0.00001 :\n",
        "  print(\"gooooooooood result\")\n",
        "else:\n",
        "  print('bad result')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gooooooooood result\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAFYSAJ10Qyc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}